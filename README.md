# Reinforcement Learning for Human Feedback (RLHF)

This repository contains the implementation of a Reinforcement Learning with Human Feedback (RLHF) system using custom datasets. The project utilizes the trlX library for training a preference model that integrates human feedback directly into the optimization of language models.


![Screenshot_2024-08-17_at_12 25 37_AM-removebg-preview](https://github.com/user-attachments/assets/42308df3-dd27-48a7-bce1-dadb1b3530e3)

