# Reinforcement Learning for Human Feedback (RLHF)

This repository contains the implementation of a Reinforcement Learning with Human Feedback (RLHF) system using custom datasets. The project utilizes the trlX library for training a preference model that integrates human feedback directly into the optimization of language models.


![Screenshot_2024-08-17_at_12 25 37_AM-removebg-preview](https://github.com/user-attachments/assets/42308df3-dd27-48a7-bce1-dadb1b3530e3)


## Installation

To set up your environment to run this project, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/your_username/RLHF_Project.git
   cd RLHF_Project
   ```

2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```


## Usage

To run the RLHF training process, execute the main.py script:
   ```bash
   python main.py
   ```
